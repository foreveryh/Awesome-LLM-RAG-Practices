---
description: >-
  åœ¨è¿™æœŸå†…å®¹ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢åœ¨æ„å»ºRAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿæ—¶ï¼Œä½ å¯ä»¥ä½¿ç”¨çš„3ç§ä¸åŒçš„æ•°æ®ç´¢å¼•æ–¹å¼ã€‚è¿™äº›æ–¹æ³•å€¼å¾—å°è¯•ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æå‡ç³»ç»Ÿçš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚
---

# \[ç´¢å¼•ç›¸å…³] é€šè¿‡è¿™ä¸‰ç§ç´¢å¼•æ–¹æ³•æ”¹å–„ RAG Pipeline



<mark style="color:green;">æ„Ÿè°¢æ‚¨å…³æ³¨æˆ‘ä»¬å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„æ¦‚è¿°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä»½æŒ‡å—èƒ½å¤Ÿæ­ç¤ºRAGçš„å¤æ‚å·¥ä½œåŸç†ï¼Œå¹¶å±•ç¤ºå…¶åœ¨ä¸åŒç¯å¢ƒä¸‹é©æ–°ä¿¡æ¯æ£€ç´¢å’Œå“åº”ç”Ÿæˆçš„æ½œåŠ›ã€‚</mark>

<mark style="color:green;">æˆ‘ä»¬å·²ç»å»ºç«‹äº†å°†RAGç³»ç»Ÿåº”ç”¨äºç”Ÿäº§ç¯å¢ƒçš„ä¸°å¯Œç»éªŒã€‚æˆ‘ä»¬çš„ä¸“ä¸šçŸ¥è¯†æ¶µç›–äº†è¯„ä¼°ç»„ç»‡éœ€æ±‚ã€éƒ¨ç½²å®šåˆ¶åŒ–çš„ã€é«˜æ€§èƒ½çš„RAGè§£å†³æ–¹æ¡ˆã€‚</mark>

<mark style="color:green;">å¦‚æœæ‚¨æ­£åœ¨è€ƒè™‘å°†RAGç³»ç»Ÿèå…¥å…¬å¸è¿è¥ä¸­ï¼Œå¹¶éœ€è¦ä¸“ä¸šæŒ‡å¯¼ä»¥ç¡®ä¿æˆåŠŸï¼Œæˆ‘ä»¬æ„¿æ„æä¾›å¸®åŠ©ã€‚æˆ‘ä»¬å°†ååŠ©æ‚¨åœ¨è¿™ä¸ªå……æ»¡æ´»åŠ›çš„æŠ€æœ¯é¢†åŸŸä¸­å¯¼èˆªï¼Œä»¥å…¨æ–°çš„æ–¹å¼åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è§£é”æ‚¨ç»„ç»‡çš„é›†ä½“çŸ¥è¯†æ½œåŠ›ã€‚</mark>

<mark style="color:green;">å¯ä»¥æ·»åŠ æˆ‘çš„å¾®ä¿¡ï¼ˆå¤‡æ³¨RAGï¼‰ï¼Œå’¨è¯¢æ„å»ºé«˜è´¨é‡ç”³è¯·åŠ å…¥æˆ‘ä»¬çš„LLM+RAGé«˜å¯ç”¨æŠ€æœ¯ç¾¤ï¼</mark>

<figure><img src="../.gitbook/assets/WechatIMG255.jpg" alt="" width="375"><figcaption></figcaption></figure>

<figure><img src="../.gitbook/assets/f985c725-64d6-4791-93b5-8f4f52c24347_1075x934.jpg" alt=""><figcaption></figcaption></figure>

## Reminder on data indexing in typical RAGs

In the default implementation of a RAG system, documents are first split into chunks.

Then, each chunk is embedded and indexed into a vector database.

In the retrieval step, the input query is also embedded and the most similar chunks are extracted.

\
In this setup, the data (i.e. _the chunks_) we retrieve is the same as the data we index.

<figure><img src="../.gitbook/assets/53634954-a976-4fa9-b458-cee3165cb7c8_4095x1235 (1).webp" alt=""><figcaption></figcaption></figure>

This is the most natural and intuitive implementation.

ğŸš¨ However, this doesnâ€™t have to be always the case.

We can index the chunks differently to increase the performance of such systems

***

**the data we retrieve doesnâ€™t have to be the same as the data we used while indexing.**

***

## 1â€”Index chunks by their subparts ğŸ§© **é€šè¿‡å­éƒ¨åˆ†ç´¢å¼•å—**&#x20;

Instead of indexing the whole chunk directly, we can split it again into smaller pieces (e.g. sentences) and index it with those multiple times.

æˆ‘ä»¬å¯ä»¥ä¸ç›´æ¥å¯¹æ•´ä¸ªå†…å®¹å—è¿›è¡Œç´¢å¼•ï¼Œè€Œæ˜¯å°†å…¶å†æ¬¡åˆ†å‰²æˆæ›´å°çš„ç‰‡æ®µï¼ˆä¾‹å¦‚ï¼Œå¥å­ï¼‰ï¼Œå¹¶å¤šæ¬¡ä½¿ç”¨è¿™äº›ç‰‡æ®µè¿›è¡Œç´¢å¼•ã€‚

<figure><img src="../.gitbook/assets/6a3cd853-e745-4a86-a2ce-41ffc1e917cf_4380x1454.webp" alt=""><figcaption></figcaption></figure>

> **Why is this useful?**

Imagine dealing with long and complex chunks that discuss multiple topics or conflicting information. Using them in a typical RAG will likely generate noisy outputs with some irrelevant content.

æƒ³è±¡ä¸€ä¸‹å¤„ç†è®¨è®ºå¤šä¸ªä¸»é¢˜æˆ–ç›¸äº’çŸ›ç›¾ä¿¡æ¯çš„é•¿ä¸”å¤æ‚çš„å†…å®¹å—ã€‚åœ¨å…¸å‹çš„RAGç³»ç»Ÿä¸­ä½¿ç”¨è¿™äº›å†…å®¹å—å¯èƒ½ä¼šäº§ç”Ÿå¸¦æœ‰ä¸€äº›ä¸ç›¸å…³å†…å®¹çš„å˜ˆæ‚è¾“å‡ºã€‚

If we separate these chunks into smaller sentences, each sentence will likely have a clear and well-defined topic that matches the user query more accurately.

å¦‚æœæˆ‘ä»¬å°†è¿™äº›å†…å®¹å—åˆ†å‰²æˆæ›´å°çš„å¥å­ï¼Œæ¯ä¸ªå¥å­å¾ˆå¯èƒ½éƒ½ä¼šæœ‰ä¸€ä¸ªæ¸…æ™°ä¸”å®šä¹‰æ˜ç¡®çš„ä¸»é¢˜ï¼Œæ›´å‡†ç¡®åœ°åŒ¹é…ç”¨æˆ·çš„æŸ¥è¯¢ã€‚

When retrieving the chunk, this method makes sure to get a relevant context to the query and a broader context (not present in the indexing sentence) that will be useful for the LLM to generate a comprehensive answer.

å½“æ£€ç´¢è¿™ä¸ªå†…å®¹å—æ—¶ï¼Œè¿™ç§æ–¹æ³•ç¡®ä¿è·å¾—ä¸æŸ¥è¯¢ç›¸å…³çš„ä¸Šä¸‹æ–‡ï¼Œä»¥åŠæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ï¼ˆç´¢å¼•å¥å­ä¸­æ²¡æœ‰çš„ï¼‰ï¼Œè¿™å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå…¨é¢ç­”æ¡ˆéå¸¸æœ‰ç”¨ã€‚

## 2â€”Index chunks by the questions â“ï¸they answer é€šè¿‡å®ƒä»¬å›ç­”çš„é—®é¢˜å¯¹å—è¿›è¡Œç´¢å¼•

Instead of indexing the chunks directly, we can instruct the LLM to generate the questions they answer and use them for indexing. This is a simple approach.

æˆ‘ä»¬å¯ä»¥ä¸ç›´æ¥å¯¹å—è¿›è¡Œç´¢å¼•ï¼Œè€Œæ˜¯æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå®ƒä»¬å›ç­”çš„é—®é¢˜ï¼Œå¹¶ä½¿ç”¨è¿™äº›é—®é¢˜è¿›è¡Œç´¢å¼•ã€‚è¿™æ˜¯ä¸€ç§ç®€å•çš„æ–¹æ³•ã€‚

<figure><img src="../.gitbook/assets/91b71590-1d0d-4a30-abe3-592c440eca55_4361x1454.webp" alt=""><figcaption></figcaption></figure>

When submitting the query, the RAG will therefore compare it to the most relevant questions that the data answers. Then, based on these questions, it will retrieve the corresponding chunks.

å½“æäº¤æŸ¥è¯¢æ—¶ï¼ŒRAGç³»ç»Ÿä¼šå°†å…¶ä¸æ•°æ®å›ç­”çš„æœ€ç›¸å…³é—®é¢˜è¿›è¡Œæ¯”è¾ƒã€‚ç„¶åï¼ŒåŸºäºè¿™äº›é—®é¢˜ï¼Œå®ƒå°†æ£€ç´¢ç›¸åº”çš„å†…å®¹å—ã€‚

This indexing method is useful because it aligns the user's query/objective with the core content of the data.

è¿™ç§ç´¢å¼•æ–¹æ³•å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå®ƒå°†ç”¨æˆ·çš„æŸ¥è¯¢/ç›®æ ‡ä¸æ•°æ®çš„æ ¸å¿ƒå†…å®¹å¯¹é½ã€‚

If a user doesnâ€™t formulate a very clear question, such an indexing method can reduce ambiguity. Instead of trying to figure out what chunks are relevant to the userâ€™s question, we directly map it to existing questions that we know we have an answer for.

å¦‚æœç”¨æˆ·æ²¡æœ‰æå‡ºä¸€ä¸ªéå¸¸æ˜ç¡®çš„é—®é¢˜ï¼Œè¿™æ ·çš„ç´¢å¼•æ–¹æ³•å¯ä»¥å‡å°‘æ­§ä¹‰ã€‚æˆ‘ä»¬ä¸æ˜¯è¯•å›¾å¼„æ¸…æ¥šå“ªäº›å†…å®¹å—ä¸ç”¨æˆ·çš„é—®é¢˜ç›¸å…³ï¼Œè€Œæ˜¯ç›´æ¥å°†å…¶æ˜ å°„åˆ°æˆ‘ä»¬çŸ¥é“æœ‰ç­”æ¡ˆçš„ç°æœ‰é—®é¢˜ä¸Šã€‚

## 3â€”Index chunks by their summaries ğŸ“ é€šè¿‡å®ƒä»¬çš„æ‘˜è¦ç´¢å¼•å—

This indexing method is similar to the previous one. It uses the chunk summary for indexing instead of the questions it answers.

è¿™ç§ç´¢å¼•æ–¹æ³•ä¸å‰ä¸€ç§ç›¸ä¼¼ã€‚å®ƒä½¿ç”¨å—çš„æ‘˜è¦è¿›è¡Œç´¢å¼•ï¼Œè€Œä¸æ˜¯å®ƒå›ç­”çš„é—®é¢˜ã€‚

<figure><img src="../.gitbook/assets/540a3f56-e9a0-47e3-bb76-aa63dc1ccf40_4598x1517.webp" alt=""><figcaption></figcaption></figure>

This is typically useful when the chunks have redundant information or irrelevant details that are not useful to the userâ€™s query. It also captures the essence of the information that the user is looking for.

å½“å—åŒ…å«å†—ä½™ä¿¡æ¯æˆ–ä¸ç”¨æˆ·æŸ¥è¯¢æ— å…³çš„ç»†èŠ‚æ—¶ï¼Œè¿™ç§æ–¹æ³•é€šå¸¸å¾ˆæœ‰ç”¨ã€‚å®ƒè¿˜æ•è·äº†ç”¨æˆ·æ­£åœ¨å¯»æ‰¾çš„ä¿¡æ¯çš„æœ¬è´¨ã€‚

***

## Appendix  è¡¥å……

Both Llama Index node parsers and Langchain sentence splitter deal with splitting the sentence into chunks but they have different scopes and functionalities:

**Llama Index Node Parsers:** Convert document into individual â€œnodesâ€ for indexing and search. They establish relationships between these nodes, providing context for the information.

**Langchain Sentence Splitters:** Divide text into individual sentences primarily for language processing tasks like translation, summarization, and sentimental analysis

åœ¨ä½¿ç”¨å„ç§ç´¢å¼•æ–¹æ³•æ”¹è¿›RAGç®¡é“æ—¶ï¼ŒLangChainçš„çµæ´»æ€§å…è®¸å¯¹åƒç´¢å¼•å­éƒ¨åˆ†ã€é—®é¢˜æˆ–æ‘˜è¦è¿™æ ·çš„æŠ€æœ¯è¿›è¡Œåˆ›æ–°å®ç°ã€‚å…¶æ¨¡å—åŒ–ç‰¹æ€§æ„å‘³ç€æ‚¨å¯ä»¥å°†å…¶é…ç½®ä¸ºå°†æ–‡æœ¬åˆ†è§£æˆæ›´æ˜“ç®¡ç†çš„ç‰‡æ®µï¼Œæˆ–è€…ä½¿ç”¨æ‘˜è¦å’Œç”Ÿæˆçš„é—®é¢˜ä½œä¸ºç´¢å¼•çš„åŸºç¡€ã€‚LlamaIndexåœ¨é«˜æ•ˆæ•°æ®æ£€ç´¢æ–¹é¢çš„ä¸“æ³¨ä½¿å…¶åœ¨åŸºäºç‰¹å®šé—®é¢˜ç´¢å¼•å—æ–¹é¢ç‰¹åˆ«æ“…é•¿ï¼Œæé«˜äº†ç»™å®šç”¨æˆ·æŸ¥è¯¢æ£€ç´¢åˆ°çš„æ–‡æ¡£çš„ç›¸å…³æ€§ã€‚é€‰æ‹©LangChainå’ŒLlamaIndexå¯èƒ½å–å†³äºæ‚¨çš„é¡¹ç›®ç‰¹å®šéœ€æ±‚ä»¥åŠæ‚¨å¸Œæœ›ä¼˜åŒ–RAGç®¡é“çš„å“ªäº›æ–¹é¢â€‹ã€‚è¯¦ç»†å†…å®¹è¯·é˜…è¯»ä»¥ä¸‹æ–‡ç« ã€‚

{% embed url="https://www.useready.com/blog/rag-wars-llama-index-vs-langchain-showdown/" %}
