---
description: >-
  è¿™æ˜¯æ¥è‡ªåŒæµçš„ä¸€ç¯‡è®ºæ–‡ã€‚å®ƒæ¶µç›–äº†å…³äºRAGæ¡†æ¶åŠå…¶å±€é™æ€§çš„æ‰€æœ‰ä½ éœ€è¦çŸ¥é“çš„å†…å®¹ã€‚å®ƒè¿˜åˆ—å‡ºäº†ä¸€äº›ç°ä»£æŠ€æœ¯ï¼Œç”¨ä»¥æå‡å…¶åœ¨æ£€ç´¢ã€å¢å¼ºå’Œç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚
  è¿™äº›æŠ€æœ¯èƒŒåçš„ç»ˆæç›®æ ‡æ˜¯ä½¿è¿™ä¸ªæ¡†æ¶å‡†å¤‡å¥½è¿›è¡Œå¯æ‰©å±•æ€§å’Œç”Ÿäº§ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé‚£äº›ç­”æ¡ˆè´¨é‡*éå¸¸*é‡è¦çš„ç”¨ä¾‹å’Œè¡Œä¸šã€‚
  æˆ‘ä¸ä¼šåœ¨è¿™ç¯‡æ–‡ç« ä¸­è®¨è®ºæ‰€æœ‰å†…å®¹ï¼Œä½†ä»¥ä¸‹æ˜¯æˆ‘è®¤ä¸ºä¼šä½¿ä½ çš„RAGæ›´é«˜æ•ˆçš„å…³é”®æ€è·¯ã€‚
---

# \[æ¨è]9ç§æœ‰æ•ˆæŠ€æœ¯æé«˜RAGç³»ç»Ÿçš„è´¨é‡

<mark style="color:green;">æ„Ÿè°¢æ‚¨å…³æ³¨æˆ‘ä»¬å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„æ¦‚è¿°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä»½æŒ‡å—èƒ½å¤Ÿæ­ç¤ºRAGçš„å¤æ‚å·¥ä½œåŸç†ï¼Œå¹¶å±•ç¤ºå…¶åœ¨ä¸åŒç¯å¢ƒä¸‹é©æ–°ä¿¡æ¯æ£€ç´¢å’Œå“åº”ç”Ÿæˆçš„æ½œåŠ›ã€‚</mark>

<mark style="color:green;">æˆ‘ä»¬å·²ç»å»ºç«‹äº†å°†RAGç³»ç»Ÿåº”ç”¨äºç”Ÿäº§ç¯å¢ƒçš„ä¸°å¯Œç»éªŒã€‚æˆ‘ä»¬çš„ä¸“ä¸šçŸ¥è¯†æ¶µç›–äº†è¯„ä¼°ç»„ç»‡éœ€æ±‚ã€éƒ¨ç½²å®šåˆ¶åŒ–çš„ã€é«˜æ€§èƒ½çš„RAGè§£å†³æ–¹æ¡ˆã€‚</mark>

<mark style="color:green;">å¦‚æœæ‚¨æ­£åœ¨è€ƒè™‘å°†RAGç³»ç»Ÿèå…¥å…¬å¸è¿è¥ä¸­ï¼Œå¹¶éœ€è¦ä¸“ä¸šæŒ‡å¯¼ä»¥ç¡®ä¿æˆåŠŸï¼Œæˆ‘ä»¬æ„¿æ„æä¾›å¸®åŠ©ã€‚æˆ‘ä»¬å°†ååŠ©æ‚¨åœ¨è¿™ä¸ªå……æ»¡æ´»åŠ›çš„æŠ€æœ¯é¢†åŸŸä¸­å¯¼èˆªï¼Œä»¥å…¨æ–°çš„æ–¹å¼åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è§£é”æ‚¨ç»„ç»‡çš„é›†ä½“çŸ¥è¯†æ½œåŠ›ã€‚</mark>

<mark style="color:green;">å¯ä»¥æ·»åŠ æˆ‘çš„å¾®ä¿¡ï¼ˆå¤‡æ³¨RAGï¼‰ï¼Œå’¨è¯¢æ„å»ºé«˜è´¨é‡ç”³è¯·åŠ å…¥æˆ‘ä»¬çš„LLM+RAGé«˜å¯ç”¨æŠ€æœ¯ç¾¤ï¼</mark>

<figure><img src="../.gitbook/assets/WechatIMG255.jpg" alt="" width="375"><figcaption></figcaption></figure>





é˜…è¯»è®ºæ–‡ [https://arxiv.org/pdf/2312.10997.pdf](https://arxiv.org/pdf/2312.10997.pdf)

### 1â€”ğŸ—ƒï¸ **Enhance the quality of indexed data æé«˜ç´¢å¼•æ•°æ®çš„è´¨é‡ï¼ˆåå¤æåŠï¼‰**

As the data we index determines the quality of the RAGâ€™s answers, the first task is to curate it as much as possible before ingesting it. _(**Garbage in, garbage out still applies here**)_\
You can do this by removing duplicate/redundant information, spotting irrelevant documents, and checking for fact accuracy (if possible).\
If the maintainability of the RAG matters, you also need to add mechanisms to refresh outdated documents.\
\
Cleaning the data is a step that is often neglected when building RAGs, as we usually tend to pour in all the documents we have without verifying their quality.

Here are some quick fixes that I suggest you go through:

* Remove text noise by cleaning special characters, weird encodings, unnecessary HTML tagsâ€¦ Remember that old NLP techniques using regex? You can reuse them.
* Spot document outliers that are irrelevant to the main topics and remove them. You can do this by implementing some topic extraction, dimensionality reduction techniques, and data visualization.
* Remove redundant documents by using similarity metrics

### 2â€”ğŸ› ï¸ **Optimize index structure ä¼˜åŒ–ç´¢å¼•ç»“æ„**

When constructing your RAG, the chunk size is a key parameter. It determines the length of the documents we retrieve from the vector store.\
**A small chunk size might result in documents that miss some crucial information while a large chunk size can introduce irrelevant noise**.

Coming up with the optimal chunk size is about finding the right balance.\
\
**How to do that efficiently? Trial and error.**

However, this doesnâ€™t mean that you have to make some random guesses and perform qualitative assessments for every experience.

You can find that optimal chunk size by running evaluations on a test set and computing metrics. LlamaIndex has interesting features to do this. You can read more about that in their [blog](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5).

### **3â€”ğŸ·ï¸ Add metadata æ·»åŠ å…ƒæ•°æ®**

Incorporating metadata with the indexed vectors helps better structure them while improving search relevance.

Here are some scenarios where metadata is useful:

* If you search for items and recency is a criterion, you can sort over a date metadata
* If you search over scientific papers and you know in advance that the information youâ€™re looking for is always located in a specific section, say the experiment section for example, you can add the article section as metadata for each chunk and filter on it to match experiments only

Metadata is useful because it brings an additional layer of structured search on top vector search.

### 4â€”â†”ï¸ Align the input query with the documents **ä½¿è¾“å…¥æŸ¥è¯¢ä¸æ–‡æ¡£å¯¹é½**

LLMs and RAGs are powerful because they offer the flexibility of expressing your query in natural language, thus lowering the entry barrier for data exploration and more complex tasks. (Learn [here](https://thetechbuffet.substack.com/p/turn-complex-english-instructions-into-sql) how RAGs can help generate SQL code from plain English instructions).

LLMå’ŒRAGä¹‹æ‰€ä»¥å¼ºå¤§ï¼Œæ˜¯å› ä¸ºå®ƒä»¬æä¾›äº†ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾æŸ¥è¯¢çš„çµæ´»æ€§ï¼Œä»è€Œé™ä½äº†æ•°æ®æ¢ç´¢å’Œæ›´å¤æ‚ä»»åŠ¡çš„å…¥é—¨é—¨æ§›ã€‚ï¼ˆåœ¨è¿™é‡Œäº†è§£RAGå¦‚ä½•å¸®åŠ©ä»ç®€å•çš„è‹±æ–‡æŒ‡ä»¤ç”ŸæˆSQLä»£ç ï¼‰ã€‚

Sometimes, however, a misalignment appears between the input query the user formulates in the form of a few words or a short sentence, and the indexed documents, which are often written in the form of longer sentences or even paragraphs.

ç„¶è€Œï¼Œæœ‰æ—¶ç”¨æˆ·ä»¥å‡ ä¸ªè¯æˆ–ä¸€ä¸ªçŸ­å¥å½¢å¼æå‡ºçš„è¾“å…¥æŸ¥è¯¢ä¸ç´¢å¼•æ–‡æ¡£ä¹‹é—´ä¼šå‡ºç°ä¸å¯¹é½çš„æƒ…å†µï¼Œè€Œç´¢å¼•æ–‡æ¡£é€šå¸¸ä»¥è¾ƒé•¿çš„å¥å­ç”šè‡³æ®µè½çš„å½¢å¼ç¼–å†™ã€‚&#x20;

Letâ€™s go through an example to understand this.

Hereâ€™s a paragraph about the motor engine:

> _The motor engine stands as an engineering marvel, propelling countless vehicles and machinery with its intricate design and mechanical prowess. At its core, a motor engine converts fuel into mechanical energy through a precisely orchestrated series of combustion events. This process involves the synchronized movement of pistons, a crankshaft, and a complex network of valves, all carefully calibrated to optimize efficiency and power output. Modern motor engines come in various types, such as internal combustion engines and electric motors, each with its unique set of advantages and applications. The relentless pursuit of innovation continues to enhance motor engine technology, pushing the boundaries of performance, fuel efficiency, and environmental sustainability. Whether powering a car on the open road or driving industrial machinery, the motor engine remains a driving force behind the dynamic movement of our modern world._

If you formulate a simple query such as **â€œCan you tell how the motor engine works in a nutshell?**â€ and compute its cosine similarity with the paragraph you obtain the value of `0.72`

Not bad, but can we do better?

To do that, we will no longer index the paragraph by its embedding but with the embeddings of the questions it answers instead.

Letâ€™s consider these three questions the paragraph answers.

1. What is the fundamental function of a motor engine?",
2. How does a motor engine convert fuel into mechanical energy?",
3. What are some key components involved in the operation of a motor engine, and how do they contribute to its efficiency?"

If we compute their similarity with the input query, we obtain these values, respectively.

1. **0.864**
2. 0.841
3. 0.845

These values are higher and indicate that the input query matches the questions more precisely.

Indexing the chunks with the questions they answer changes the problem slightly but helps address alignment problems and increases search relevance: we donâ€™t optimize for the similarity with the documents but with underlying questions.

è¿™äº›å€¼æ›´é«˜ï¼Œè¡¨æ˜è¾“å…¥æŸ¥è¯¢ä¸é—®é¢˜çš„åŒ¹é…æ›´åŠ ç²¾ç¡®ã€‚ç”¨å®ƒä»¬å›ç­”çš„é—®é¢˜å¯¹å—è¿›è¡Œç´¢å¼•è™½ç„¶ç•¥å¾®æ”¹å˜äº†é—®é¢˜ï¼Œä½†æœ‰åŠ©äºè§£å†³å¯¹é½é—®é¢˜å¹¶å¢åŠ æœç´¢ç›¸å…³æ€§ï¼šæˆ‘ä»¬ä¸æ˜¯ä¼˜åŒ–ä¸æ–‡æ¡£çš„ç›¸ä¼¼æ€§ï¼Œè€Œæ˜¯ä¸åº•å±‚é—®é¢˜çš„ç›¸ä¼¼æ€§ã€‚

### 5â€”ğŸ” Mixed retrieval **æ··åˆæ£€ç´¢**

While vector search helps retrieve semantically relevant chunks for a given query, it occasionally lacks precision in matching specific keywords.

Depending on the use case, an exact match can be sometimes necessary.

Imagine searching a vector database of millions of e-commerce products, and to the query â€œAdidas ref XYZ sneakers whiteâ€ the top results include white Adidas sneakers with none matching the exact XYZ reference.

This would be rather disappointing.

To tackle this issue, mixed retrieval is a solution. This strategy leverages the strengths of different retrieval technologies such as vector search and keyword search and combines them intelligently.

With this hybrid approach, you can still match relevant keywords while maintaining control over the query intent.

Check Pineconeâ€™s starter [guide](https://www.pinecone.io/learn/hybrid-search-intro/) to learn more about hybrid search.

### 6â€”ğŸ”„ ReRank **é‡æ–°æ’åº**

When you query the vectorstore, the top K results are not necessarily ordered in the most relevant way. Granted theyâ€™re all relevant but the most relevant chunk among these relevant chunks can be number #5 or #7 instead of #1 or #2

Hereâ€™s where ReRank comes in.

The straightforward concept of re-ranking to relocate the most pertinent information toward the edges of the prompt has been successfully implemented in various frameworks, including LlamaIndex, LangChain, and HayStack.

For example, Diversity Ranker focuses on reordering based on document diversity, while LostInTheMiddleRanker alternates the placement of the best document at both the beginning and end of the context window.

### 7â€”ğŸ—œï¸ Prompt compression **æç¤ºå‹ç¼©**

Itâ€™s been shown that noise in the retrieved contexts adversely affects the RAG performance, more precisely, the answer generated by the LLM.

Some suggested techniques apply a post-processing step after retrieval to compress irrelevant context, highlight important paragraphs, and reduce the overall context length.

Approaches like Selective Context \[[_Litman et. al_](https://arxiv.org/abs/2003.11288)] and LLMLingua \[[Aderson et. al](https://arxiv.org/abs/2310.05736)] use small LLMs to calculate prompt mutual information or perplexity, thus estimating element importance.

### 8â€”ğŸ’¡ HyDE

This method comes from this [paper](https://arxiv.org/abs/2212.10496) and stands for **Hy**pothetical **D**ocument **E**mbedding.

When presented with a query, HyDE instructs an LLM to generate a hypothetical answer.

This document, while capturing relevance patterns, is not real and may contain inaccuracies. Subsequently, an unsupervised contrastively learned encoder (e.g., Contriever) transforms the document into an embedding vector.

This vector serves to pinpoint a neighborhood in the corpus embedding space, enabling the retrieval of similar real documents based on vector similarity. In this second step, the generated document is anchored to the actual corpus, with the encoder's dense bottleneck effectively filtering out incorrect details.

Experiments demonstrate that HyDE consistently outperforms the state-of-the-art unsupervised dense retriever Contriever and exhibits robust performance comparable to fine-tuned retrievers across various tasks (such as web search, QA, and fact verification) and languages.

### 9â€”âœ’ï¸ Query rewrite and expansion **æŸ¥è¯¢é‡å†™å’Œæ‰©å±•**

When a user interacts with a RAG, his query is not necessarily well-formulated and doesn't fully express an intent that can be efficiently matched with documents in the vectorstore.

To solve this issue, we instruct an LLM to rewrite this query behind the scenes before sending it to the RAG.

This can be easily implemented by adding intermediate LLM calls but other sophisticated techniques (reviewed in this [paper](https://arxiv.org/abs/2305.03653)) exist.

***

## Appendix æ•™ç¨‹**è¡¥å……**

LlamaIndex offers a range of solutions that can address several of the 9 techniques for enhancing Retrieval Augmented Generation (RAG) systems. Here's how LlamaIndex can help:

1. **Data Quality and Indexing**: LlamaIndex provides tools for curating and cleaning data before indexing, which is crucial for maintaining high-quality RAG systems. The platform allows for the removal of duplicate, redundant, or irrelevant information, ensuring that the indexed data is as relevant and up-to-date as possible.
2. **Optimal Chunking Strategy**: The platform offers various node parsers like SentenceSplitter and CodeSplitter, which are essential for breaking down documents into manageable chunks. This facilitates better handling and processing of documents, allowing for more effective indexing and retrieval.
3. **Vector Store Optimization**: LlamaIndex supports the selection of an appropriate vector store, a critical component for storing and retrieving embeddings in the RAG pipeline. This ensures efficient management of vector data, which is key to the performance of RAG systems.
4. **Response Synthesis**: The platform provides various response synthesis methods, such as refine and tree\_summarize, allowing users to choose how the RAG pipeline synthesizes responses. This flexibility can enhance the relevance and accuracy of the generated answers.

LlamaIndex's comprehensive suite of tools and features makes it a valuable asset for building and optimizing RAG systems, particularly for applications where answer quality is paramount. The platform's focus on data quality, effective chunking, vector store optimization, and flexible response synthesis aligns well with the key techniques for boosting RAG systems.

For more detailed information on how LlamaIndex supports these aspects, you can visit their official documentation and guides:

* For an overview of evaluating RAG systems using LlamaIndex: [LlamaIndex Blog](https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93)
* For details on building a no-code RAG pipeline with LlamaIndex: [RAGArch by LlamaIndex](https://www.llamaindex.ai)

LlamaIndexä¸ºæå‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿæä¾›äº†å¤šç§è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥åº”å¯¹å‰é¢æåˆ°çš„9ç§å¢å¼ºæŠ€æœ¯ä¸­çš„å‡ ç§ã€‚ä»¥ä¸‹æ˜¯LlamaIndexå¦‚ä½•å¸®åŠ©çš„ä¸€äº›æ–¹å¼ï¼š

1. **æ•°æ®è´¨é‡å’Œç´¢å¼•**ï¼šLlamaIndexæä¾›äº†åœ¨ç´¢å¼•ä¹‹å‰ç­–åˆ’å’Œæ¸…ç†æ•°æ®çš„å·¥å…·ï¼Œè¿™å¯¹äºç»´æŠ¤é«˜è´¨é‡çš„RAGç³»ç»Ÿè‡³å…³é‡è¦ã€‚å¹³å°å…è®¸å»é™¤é‡å¤ã€å†—ä½™æˆ–ä¸ç›¸å…³çš„ä¿¡æ¯ï¼Œç¡®ä¿ç´¢å¼•æ•°æ®å°½å¯èƒ½ç›¸å…³å’Œæœ€æ–°ã€‚
2. **æœ€ä½³åˆ†å—ç­–ç•¥**ï¼šè¯¥å¹³å°æä¾›äº†å„ç§èŠ‚ç‚¹è§£æå™¨ï¼Œå¦‚SentenceSplitterå’ŒCodeSplitterï¼Œè¿™äº›å¯¹äºå°†æ–‡æ¡£åˆ†è§£æˆæ˜“äºç®¡ç†çš„å—è‡³å…³é‡è¦ã€‚è¿™æœ‰åŠ©äºæ›´å¥½åœ°å¤„ç†å’Œå¤„ç†æ–‡æ¡£ï¼Œå…è®¸æ›´æœ‰æ•ˆçš„ç´¢å¼•å’Œæ£€ç´¢ã€‚
3. **å‘é‡å­˜å‚¨ä¼˜åŒ–**ï¼šLlamaIndexæ”¯æŒé€‰æ‹©é€‚å½“çš„å‘é‡å­˜å‚¨ï¼Œè¿™æ˜¯å­˜å‚¨å’Œæ£€ç´¢RAGç®¡é“ä¸­åµŒå…¥çš„å…³é”®ç»„ä»¶ã€‚è¿™ç¡®ä¿äº†å‘é‡æ•°æ®çš„æœ‰æ•ˆç®¡ç†ï¼Œè¿™æ˜¯RAGç³»ç»Ÿæ€§èƒ½çš„å…³é”®ã€‚
4. **å“åº”åˆæˆ**ï¼šè¯¥å¹³å°æä¾›äº†å„ç§å“åº”åˆæˆæ–¹æ³•ï¼Œå¦‚refineå’Œtree\_summarizeï¼Œå…è®¸ç”¨æˆ·é€‰æ‹©RAGç®¡é“å¦‚ä½•åˆæˆå“åº”ã€‚è¿™ç§çµæ´»æ€§å¯ä»¥å¢å¼ºç”Ÿæˆç­”æ¡ˆçš„ç›¸å…³æ€§å’Œå‡†ç¡®æ€§ã€‚

LlamaIndexçš„å…¨é¢å·¥å…·å’ŒåŠŸèƒ½å¥—ä»¶ä½¿å…¶æˆä¸ºæ„å»ºå’Œä¼˜åŒ–RAGç³»ç»Ÿçš„å®è´µèµ„äº§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç­”æ¡ˆè´¨é‡è‡³å…³é‡è¦çš„åº”ç”¨ã€‚å¹³å°å¯¹æ•°æ®è´¨é‡ã€æœ‰æ•ˆåˆ†å—ã€å‘é‡å­˜å‚¨ä¼˜åŒ–å’Œçµæ´»å“åº”åˆæˆçš„å…³æ³¨ä¸æå‡RAGç³»ç»Ÿçš„å…³é”®æŠ€æœ¯ç›¸å»åˆã€‚

æœ‰å…³LlamaIndexå¦‚ä½•æ”¯æŒè¿™äº›æ–¹é¢çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥è®¿é—®å®˜æ–¹æ–‡æ¡£å’ŒæŒ‡å—ï¼š

* æœ‰å…³ä½¿ç”¨LlamaIndexè¯„ä¼°RAGç³»ç»Ÿçš„æ¦‚è§ˆï¼Œè¯·è®¿é—®ï¼š[LlamaIndexåšå®¢](https://www.llamaindex.ai/blog/openai-cookbook-evaluating-rag-systems-fe393c61fb93)
* æœ‰å…³ä½¿ç”¨LlamaIndexæ„å»ºæ— ä»£ç RAGç®¡é“çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®ï¼š[LlamaIndexçš„RAGArch](https://www.llamaindex.ai)ã€‚

å…³äºå¦‚ä½•æ„å»ºé«˜è´¨é‡RAGåº”ç”¨ï¼Œå¯ä»¥æ·»åŠ æˆ‘çš„å¾®ä¿¡ï¼ˆè¿›å…¥é«˜è´¨é‡RAGäº§å“æŠ€æœ¯ç¾¤ï¼‰è·å–æœ€æ–°ä¿¡æ¯ã€‚

<figure><img src="../.gitbook/assets/WechatIMG255.jpg" alt=""><figcaption></figcaption></figure>
