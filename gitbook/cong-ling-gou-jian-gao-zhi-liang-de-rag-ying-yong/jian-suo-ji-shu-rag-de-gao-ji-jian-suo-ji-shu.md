---
description: >-
  è¯¥æ–‡å°†ä»‹ç»ä¸‰ç§é«˜æ•ˆæ”¹è¿›åŸºäº RAG (æ£€ç´¢å¼ç”Ÿæˆ) åº”ç”¨ä¸­æ–‡æ¡£æ£€ç´¢çš„å¼ºå¤§æŠ€æœ¯ã€‚åˆ†åˆ«æ˜¯æŸ¥è¯¢æ‰©å±•ï¼ˆQuery
  expansionï¼‰ã€äº¤å‰ç¼–ç å™¨é‡æ’åºï¼ˆCross-encoder re-rankingï¼‰å’ŒåµŒå…¥é€‚é…å™¨ï¼ˆEmbedding
  adaptorsï¼‰ã€‚é€šè¿‡å®æ–½è¿™äº›æŠ€æœ¯ï¼Œä½ å°†æ£€ç´¢åˆ°æ›´åŠ ç›¸å…³çš„æ–‡æ¡£ï¼Œè¿™äº›æ–‡æ¡£æ›´å¥½åœ°ä¸ç”¨æˆ·æŸ¥è¯¢å¯¹é½ï¼Œä»è€Œä½¿ç”Ÿæˆçš„ç­”æ¡ˆæ›´å…·å½±å“åŠ›ã€‚
---

# \[æ£€ç´¢æŠ€æœ¯] RAGçš„é«˜çº§æ£€ç´¢æŠ€æœ¯

<mark style="color:green;">æ„Ÿè°¢æ‚¨å…³æ³¨æˆ‘ä»¬å¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„æ¦‚è¿°ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä»½æŒ‡å—èƒ½å¤Ÿæ­ç¤ºRAGçš„å¤æ‚å·¥ä½œåŸç†ï¼Œå¹¶å±•ç¤ºå…¶åœ¨ä¸åŒç¯å¢ƒä¸‹é©æ–°ä¿¡æ¯æ£€ç´¢å’Œå“åº”ç”Ÿæˆçš„æ½œåŠ›ã€‚</mark>

<mark style="color:green;">æˆ‘ä»¬å·²ç»å»ºç«‹äº†å°†RAGç³»ç»Ÿåº”ç”¨äºç”Ÿäº§ç¯å¢ƒçš„ä¸°å¯Œç»éªŒã€‚æˆ‘ä»¬çš„ä¸“ä¸šçŸ¥è¯†æ¶µç›–äº†è¯„ä¼°ç»„ç»‡éœ€æ±‚ã€éƒ¨ç½²å®šåˆ¶åŒ–çš„ã€é«˜æ€§èƒ½çš„RAGè§£å†³æ–¹æ¡ˆã€‚</mark>

<mark style="color:green;">å¦‚æœæ‚¨æ­£åœ¨è€ƒè™‘å°†RAGç³»ç»Ÿèå…¥å…¬å¸è¿è¥ä¸­ï¼Œå¹¶éœ€è¦ä¸“ä¸šæŒ‡å¯¼ä»¥ç¡®ä¿æˆåŠŸï¼Œæˆ‘ä»¬æ„¿æ„æä¾›å¸®åŠ©ã€‚æˆ‘ä»¬å°†ååŠ©æ‚¨åœ¨è¿™ä¸ªå……æ»¡æ´»åŠ›çš„æŠ€æœ¯é¢†åŸŸä¸­å¯¼èˆªï¼Œä»¥å…¨æ–°çš„æ–¹å¼åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è§£é”æ‚¨ç»„ç»‡çš„é›†ä½“çŸ¥è¯†æ½œåŠ›ã€‚</mark>

<mark style="color:green;">å¯ä»¥æ·»åŠ æˆ‘çš„å¾®ä¿¡ï¼ˆå¤‡æ³¨RAGï¼‰ï¼Œå’¨è¯¢æ„å»ºé«˜è´¨é‡ç”³è¯·åŠ å…¥æˆ‘ä»¬çš„LLM+RAGé«˜å¯ç”¨æŠ€æœ¯ç¾¤ï¼</mark>

<figure><img src="../.gitbook/assets/WechatIMG255.jpg" alt="" width="375"><figcaption></figcaption></figure>

<figure><img src="../.gitbook/assets/f985c725-64d6-4791-93b5-8f4f52c24347_1075x934.jpg" alt=""><figcaption></figcaption></figure>

å…ˆè´´ä¸ŠRAGçš„è¿™å¼ å›¾ï¼Œå®ƒæ­ç¤ºäº†ç´¢å¼•ï¼ˆIndex)å’Œæ£€ç´¢ï¼ˆRetrievalï¼‰è¿‡ç¨‹ï¼Œè™½ç„¶ç•¥æ˜¾å¤æ‚ï¼Œä½†å¯¹æ•´ä¸ªè¿‡ç¨‹çš„ç†è§£è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»çš„å†…å®¹å‡å‘ç”Ÿåœ¨Retrievalé˜¶æ®µã€‚

## 1â€” Query expansion æŸ¥è¯¢æ‰©å±• <a href="#sr-toc-0" id="sr-toc-0"></a>

Query expansion refers to a set of techniques that rephrase the original query.

æŸ¥è¯¢æ‰©å±•æŒ‡çš„æ˜¯ä¸€ç»„å¯¹åŸå§‹æŸ¥è¯¢è¿›è¡Œæ”¹å†™çš„æŠ€æœ¯ã€‚

Weâ€™ll look at two methods:

### **ğŸ‘‰ Query expansion with a generated answer**

å€ŸåŠ©ç”Ÿæˆçš„ç­”æ¡ˆè¿›è¡ŒæŸ¥è¯¢æ‰©å±•

Given an input query, this method first instructs an LLM to provide a hypothetical answer, whatever its correctness.

ç»™å®šä¸€ä¸ªè¾“å…¥æŸ¥è¯¢ï¼Œæ­¤æ–¹æ³•é¦–å…ˆæŒ‡ç¤ºä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹æä¾›ä¸€ä¸ªå‡è®¾æ€§çš„ç­”æ¡ˆï¼Œä¸è®ºå…¶æ­£ç¡®ä¸å¦ã€‚

Then, the query and the generated answer are combined in a prompt and sent to the retrieval system.

ç„¶åï¼Œå°†æŸ¥è¯¢å’Œç”Ÿæˆçš„ç­”æ¡ˆç»„åˆåœ¨ä¸€ä¸ªæç¤ºä¸­ï¼Œå‘é€ç»™æ£€ç´¢ç³»ç»Ÿã€‚

<figure><img src="../.gitbook/assets/ac06b3c5-b0b2-44aa-aad0-6f107df1a578_683x661.jpg" alt=""><figcaption></figcaption></figure>

> This technique **surprisingly works well**. Check the findings of this [paper](https://arxiv.org/abs/2212.10496) to learn more about it. åŒæ ·æ¥è‡ªå›½äººçš„ä¸€ç¯‡è®ºæ–‡

The rationale behind this method is that we want to retrieve documents that look more like an answer. The correctness of the hypothetical answer doesnâ€™t matter much because what weâ€™re really interested in is its structure and formulation.

è¿™ç§æ–¹æ³•èƒŒåçš„åŸºæœ¬åŸç†æ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›æ£€ç´¢åˆ°æ›´åƒç­”æ¡ˆçš„æ–‡æ¡£ã€‚å‡è®¾æ€§ç­”æ¡ˆçš„æ­£ç¡®æ€§å¹¶ä¸æ˜¯å¾ˆé‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬çœŸæ­£æ„Ÿå…´è¶£çš„æ˜¯å®ƒçš„ç»“æ„å’Œè¡¨è¿°æ–¹å¼ã€‚

At best, you could consider the hypothetical answer as a template that helps identify a relevant neighborhood in the embedding space.

åœ¨æœ€ä½³æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥å°†å‡è®¾æ€§ç­”æ¡ˆè§†ä¸ºä¸€ä¸ªæ¨¡æ¿ï¼Œå®ƒæœ‰åŠ©äºåœ¨åµŒå…¥ç©ºé—´ä¸­è¯†åˆ«å‡ºä¸€ä¸ªç›¸å…³çš„é‚»åŸŸã€‚

Hereâ€™s an example of a prompt I used to augment the query for a RAG that answers questions about financial reports.

è¿™é‡Œæœ‰ä¸€ä¸ªæˆ‘ç”¨æ¥å¢å¼º RAG æŸ¥è¯¢çš„ç¤ºä¾‹ï¼Œè¿™ä¸ª RAG ç”¨äºå›ç­”æœ‰å…³è´¢åŠ¡æŠ¥å‘Šçš„é—®é¢˜ã€‚

```
You are a helpful expert financial research assistant. 

Provide an example answer to the given question, that might be found in a document like an annual report.
```

### **ğŸ‘‰ Query expansion with multiple related questions**

This second method instructs an LLM to generate N questions related to the original query and then sends them all (+ the original query) to the retrieval system.

è¿™ç¬¬äºŒç§æ–¹æ³•æŒ‡ç¤ºä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆ N ä¸ªä¸åŸå§‹æŸ¥è¯¢ç›¸å…³çš„é—®é¢˜ï¼Œç„¶åå°†å®ƒä»¬å…¨éƒ¨ï¼ˆ+åŸå§‹æŸ¥è¯¢ï¼‰å‘é€åˆ°æ£€ç´¢ç³»ç»Ÿã€‚

By doing this, more documents will be retrieved from the vectorstore. However, some of them will be duplicates which is why you need to perform post-processing.

é€šè¿‡è¿™æ ·åšï¼Œå°†ä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢åˆ°æ›´å¤šæ–‡æ¡£ã€‚ç„¶è€Œï¼Œå…¶ä¸­ä¸€äº›æ–‡æ¡£å°†æ˜¯é‡å¤çš„ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ éœ€è¦æ‰§è¡Œåå¤„ç†çš„åŸå› ã€‚

![](../.gitbook/assets/64e67f47-4dde-4646-8e78-19847022386b\_691x682.jpg)\
The idea behind this method is that you extend the initial query that may be incomplete and incorporate related aspects that can be eventually relevant and complementary.

è¿™ç§æ–¹æ³•èƒŒåçš„æ€æƒ³æ˜¯ï¼Œä½ æ‰©å±•äº†å¯èƒ½ä¸å®Œæ•´çš„åˆå§‹æŸ¥è¯¢ï¼Œå¹¶çº³å…¥ç›¸å…³æ–¹é¢ï¼Œè¿™äº›æ–¹é¢æœ€ç»ˆå¯èƒ½æ˜¯ç›¸å…³ä¸”äº’è¡¥çš„ã€‚

Hereâ€™s a prompt I used to generate related questions:

```
You are a helpful expert financial research assistant. 
Your users are asking questions about an annual report.
Suggest up to five additional related questions to help them find the information they need, for the provided question.
Suggest only short questions without compound sentences. Suggest a variety of questions that cover different aspects of the topic.
Make sure they are complete questions, and that they are related to the original question.
Output one question per line. Do not number the questions."
```

The downside of this method is that we end up with a lot more documents that may distract the LLM from generating a useful answer.

è¿™ç§æ–¹æ³•çš„ç¼ºç‚¹æ˜¯ï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šå¾—åˆ°æ›´å¤šå¯èƒ½ä¼šåˆ†æ•£å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæœ‰ç”¨ç­”æ¡ˆæ³¨æ„åŠ›çš„æ–‡æ¡£ã€‚\


**hatâ€™s where re-ranking comes into play.**

> To learn more about different query expansion techniques, check this [paper](https://arxiv.org/abs/2305.03653) for Google.

## 2â€”Cross encoder re-ranking äº¤å‰ç¼–ç å™¨é‡æ’åº

This method re-ranks the retrieved documents according to a score that quantifies their relevancy with the input query.

è¿™ç§æ–¹æ³•æ ¹æ®ä¸€ä¸ªåˆ†æ•°å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ–°æ’åºï¼Œè¯¥åˆ†æ•°é‡åŒ–äº†å®ƒä»¬ä¸è¾“å…¥æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚\
![](../.gitbook/assets/496bd9af-490d-4428-9029-391002ba98f8\_913x927.jpg)\


To compute this score, we will use a _cross-encoder._

ä¸ºäº†è®¡ç®—è¿™ä¸ªåˆ†æ•°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªäº¤å‰ç¼–ç å™¨ã€‚

A cross-encoder is a deep neural network that processes two input sequences together as a single input. This allows the model to directly compare and contrast the inputs, understanding their relationship in a more integrated and nuanced way.

äº¤å‰ç¼–ç å™¨æ˜¯ä¸€ç§æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå®ƒå°†ä¸¤ä¸ªè¾“å…¥åºåˆ—ä¸€èµ·ä½œä¸ºå•ä¸ªè¾“å…¥è¿›è¡Œå¤„ç†ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç›´æ¥æ¯”è¾ƒå’Œå¯¹æ¯”è¾“å…¥ï¼Œä»¥æ›´åŠ ç»¼åˆå’Œç»†è‡´çš„æ–¹å¼ç†è§£å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚

<figure><img src="../.gitbook/assets/11023a3a-557b-4926-b1f3-3c905bb58382_313x345.jpg" alt=""><figcaption></figcaption></figure>

Cross-encoders can be used for Information Retrieval: given a query, encode the query with all retrieved documents. Then, sort them in a decreasing order. The high-scored documents are the most relevant ones.

äº¤å‰ç¼–ç å™¨å¯ç”¨äºä¿¡æ¯æ£€ç´¢ï¼šç»™å®šä¸€ä¸ªæŸ¥è¯¢ï¼Œå°†æŸ¥è¯¢ä¸æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸€èµ·ç¼–ç ã€‚ç„¶åï¼Œå°†å®ƒä»¬æŒ‰é™åºæ’åºã€‚å¾—åˆ†é«˜çš„æ–‡æ¡£æ˜¯æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚

> See [**SBERT.net Retrieve & Re-rank**](https://www.sbert.net/examples/applications/retrieve\_rerank/README.html) for more details.

<figure><img src="../.gitbook/assets/77b480c8-6b41-481b-ab50-e63b205eb2be_645x479.jpg" alt=""><figcaption></figcaption></figure>

Hereâ€™s how to quickly get started with re-ranking using cross-encoders:

* Install sentence-transformers:

```
pip install -U sentence-transformers
```

* Import the cross-encoder and load it:

```
from sentence_transformers import CrossEncoder
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
```

* Score each pair of (query, document):

```
pairs = [[query, doc] for doc in retrieved_documents]
scores = cross_encoder.predict(pairs)
print("Scores:")
for score in scores:
    print(score)

Scores:
0.98693466
2.644579
-0.26802942
-10.73159
-7.7066045
-5.6469955
-4.297035
-10.933233
-7.0384283
-7.3246956
```

* Reorder the documents:

```
print("New Ordering:")
for o in np.argsort(scores)[::-1]:
    print(o+1)
```

> Cross-encoder re-ranking can be used with query expansion: after you generate multiple related questions and retrieve the corresponding documents (say you end up with M documents), you re-rank them and pick the top K (K < M). That way, you reduce the context size while selecting the most important pieces.
>
> äº¤å‰ç¼–ç å™¨é‡æ’åºå¯ä»¥ä¸æŸ¥è¯¢æ‰©å±•ä¸€èµ·ä½¿ç”¨ï¼šåœ¨ä½ ç”Ÿæˆå¤šä¸ªç›¸å…³é—®é¢˜å¹¶æ£€ç´¢ç›¸åº”çš„æ–‡æ¡£ä¹‹åï¼ˆå‡è®¾ä½ æœ€ç»ˆå¾—åˆ°äº† M ä»½æ–‡æ¡£ï¼‰ï¼Œä½ å¯¹å®ƒä»¬è¿›è¡Œé‡æ’åºå¹¶é€‰æ‹©æ’åå‰ K çš„æ–‡æ¡£ï¼ˆK < Mï¼‰ã€‚è¿™æ ·ï¼Œä½ å°±å‡å°äº†ä¸Šä¸‹æ–‡å¤§å°ï¼ŒåŒæ—¶é€‰æ‹©äº†æœ€é‡è¦çš„éƒ¨åˆ†ã€‚

In the next section, weâ€™re going to dive into adaptors, a powerful yet simple-to-implement technique to scale embeddings to better align with the userâ€™s task.

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨é€‚é…å™¨ï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§è€Œç®€å•çš„æŠ€æœ¯ï¼Œç”¨äºæ‰©å±•åµŒå…¥ï¼Œä»¥æ›´å¥½åœ°ä¸ç”¨æˆ·çš„ä»»åŠ¡å¯¹é½ã€‚

## 3â€”Embedding adaptors åµŒå…¥é€‚é…å™¨

This method leverages feedback on the relevancy of the retrieved documents to train an adapter.

è¿™ç§æ–¹æ³•åˆ©ç”¨å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£ç›¸å…³æ€§çš„åé¦ˆæ¥è®­ç»ƒä¸€ä¸ªé€‚é…å™¨ã€‚

> An adapter is a lightweight alternative to fully fine-tune a pre-trained model. Currently, adapters are implemented as **small feedforward neural networks that are inserted between layers of pre-trained models.**
>
> é€‚é…å™¨æ˜¯å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå…¨é¢å¾®è°ƒçš„è½»é‡çº§æ›¿ä»£æ–¹æ¡ˆã€‚ç›®å‰ï¼Œé€‚é…å™¨è¢«å®ç°ä¸º**å°å‹å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå®ƒä»¬è¢«æ’å…¥åˆ°é¢„è®­ç»ƒæ¨¡å‹çš„å„å±‚ä¹‹é—´ã€‚**

The underlying goal of training an adapter is to alter the embedding query to produce better retrieval results for a specific task.

è®­ç»ƒé€‚é…å™¨çš„åŸºæœ¬ç›®æ ‡æ˜¯æ”¹å˜åµŒå…¥æŸ¥è¯¢ï¼Œä»¥ä¾¿ä¸ºç‰¹å®šä»»åŠ¡äº§ç”Ÿæ›´å¥½çš„æ£€ç´¢ç»“æœã€‚

An embedding adapter is a stage that can be inserted after the embedding phase and before the retrieval. Think about it as a matrix.

åµŒå…¥é€‚é…å™¨æ˜¯ä¸€ä¸ªå¯ä»¥åœ¨åµŒå…¥é˜¶æ®µä¹‹åå’Œæ£€ç´¢ä¹‹å‰æ’å…¥çš„é˜¶æ®µã€‚å¯ä»¥å°†å…¶è§†ä¸ºä¸€ä¸ªçŸ©é˜µã€‚



<figure><img src="../.gitbook/assets/3bb18b4d-dc2d-48e9-b1d6-10c98b590078_839x723.jpg" alt=""><figcaption></figcaption></figure>

To train an adapter, we need to go through the following steps:

### **Prepare the training data**

To train an embedding adapter, we need some training data on the relevancy of the documents. (it can be manually annotated or synthetic)

è¦è®­ç»ƒåµŒå…¥é€‚é…å™¨ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›å…³äºæ–‡æ¡£ç›¸å…³æ€§çš„è®­ç»ƒæ•°æ®ï¼ˆå¯ä»¥æ˜¯æ‰‹åŠ¨æ ‡æ³¨çš„æˆ–åˆæˆçš„ï¼‰ã€‚

This data must include tuples of (query, document) as well as their corresponding labels (1 if the document is relevant to the query, -1 otherwise).

è¿™äº›æ•°æ®å¿…é¡»åŒ…æ‹¬ï¼ˆæŸ¥è¯¢ï¼Œæ–‡æ¡£ï¼‰çš„å…ƒç»„åŠå…¶å¯¹åº”çš„æ ‡ç­¾ï¼ˆå¦‚æœæ–‡æ¡£ä¸æŸ¥è¯¢ç›¸å…³ï¼Œåˆ™ä¸º 1ï¼›å¦åˆ™ä¸º -1ï¼‰ã€‚

For this tutorial, weâ€™re going to create a synthetic dataset. To do that, we first generate sample questions using an LLM and this prompt:

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåˆæˆæ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œä»¥ä¸‹æç¤ºç”Ÿæˆç¤ºä¾‹é—®é¢˜ï¼š

```
You are a helpful expert financial research assistant. 
You help users analyze financial statements to better understand companies.
Suggest 10 to 15 short questions that are important to ask when analyzing an annual report.
Do not output any compound questions (questions with multiple sentences or conjunctions).
Output each question on a separate line divided by a newline.
```

Then, we retrieve documents for each generated question

We evaluate the relevance of each question to the retrieved document using an LLM with the following prompt:

```
You are a helpful expert financial research assistant. 
You help users analyze financial statements to better understand companies.
For the given query, evaluate whether the following satement is relevant.
Output only 'yes' or 'no'.
```

Then, we create tuples and corresponding labels

```
adapter_query_embeddings = []
adapter_doc_embeddings = []
adapter_labels = []

for q, query in enumerate(tqdm(generated_queries)):
    for d, document in enumerate(retrieved_documents[q]):
        adapter_query_embeddings.append(query_embeddings[q])
        adapter_doc_embeddings.append(retrieved_embeddings[q][d])
        adapter_labels.append(evaluate_results(query, document))
```

and put everything in a Torch dataset:

```
adapter_query_embeddings = torch.Tensor(np.array(adapter_query_embeddings))
adapter_doc_embeddings = torch.Tensor(np.array(adapter_doc_embeddings))
adapter_labels = torch.Tensor(np.expand_dims(np.array(adapter_labels),1))
dataset = torch.utils.data.TensorDataset(adapter_query_embeddings, adapter_doc_embeddings, adapter_labels)
```

### **Define a model**

We define a function that takes the query embedding, the document embedding, and the adaptor matrix. Then, it multiplies the query embedding with embedding matrix and computes a cosine similarity between this result and the document embedding.\
æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥å—æŸ¥è¯¢åµŒå…¥ã€æ–‡æ¡£åµŒå…¥å’Œé€‚é…å™¨çŸ©é˜µã€‚ç„¶åï¼Œå®ƒå°†æŸ¥è¯¢åµŒå…¥ä¸åµŒå…¥çŸ©é˜µç›¸ä¹˜ï¼Œå¹¶è®¡ç®—è¯¥ç»“æœä¸æ–‡æ¡£åµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

```
def model(query_embedding, document_embedding, adaptor_matrix):
    updated_query_embedding = torch.matmul(adaptor_matrix, query_embedding)
    return torch.cosine_similarity(updated_query_embedding, document_embedding, dim=0)
```

### **Define the loss**

We use MSE as the error to optimize the model.

æˆ‘ä»¬ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä½œä¸ºä¼˜åŒ–æ¨¡å‹çš„è¯¯å·®æŒ‡æ ‡ã€‚

```
def mse_loss(query_embedding, document_embedding, adaptor_matrix, label):
    return torch.nn.MSELoss()(model(query_embedding, document_embedding, adaptor_matrix), label)
```

### **Run backpropagation:**

We run 100 epochs to train the adapter matrix.

æˆ‘ä»¬è¿è¡Œ100ä¸ªå‘¨æœŸæ¥è®­ç»ƒé€‚é…å™¨çŸ©é˜µã€‚

```
# Initialize the adaptor matrix
mat_size = len(adapter_query_embeddings[0])
adapter_matrix = torch.randn(mat_size, mat_size, requires_grad=True)

min_loss = float('inf')
best_matrix = None

for epoch in tqdm(range(100)):
    for query_embedding, document_embedding, label in dataset:
        loss = mse_loss(query_embedding, document_embedding, adapter_matrix, label)

        if loss < min_loss:
            min_loss = loss
            best_matrix = adapter_matrix.clone().detach().numpy()

        loss.backward()
        with torch.no_grad():
            adapter_matrix -= 0.01 * adapter_matrix.grad
            adapter_matrix.grad.zero_()
```

When the training is complete, the adapter can now be used to scale the original embedding and adapt to the user task. All you need now is take the original embedding output and multiply it with the adaptor matrix before feeding it to the retrieval system.

å½“è®­ç»ƒå®Œæˆåï¼Œé€‚é…å™¨ç°åœ¨å¯ä»¥ç”¨æ¥æ‰©å±•åŸå§‹åµŒå…¥å¹¶é€‚åº”ç”¨æˆ·ä»»åŠ¡ã€‚ç°åœ¨ä½ éœ€è¦åšçš„å°±æ˜¯å–åŸå§‹åµŒå…¥è¾“å‡ºï¼Œå¹¶åœ¨å°†å…¶é€å…¥æ£€ç´¢ç³»ç»Ÿä¹‹å‰ï¼Œå…ˆç”¨é€‚é…å™¨çŸ©é˜µè¿›è¡Œä¹˜æ³•è¿ç®—ã€‚\


## Appendix è¡¥å……

These retrieval techniques we coverd help improve the relevancy of the documents.

æˆ‘ä»¬è®¨è®ºçš„è¿™äº›æ£€ç´¢æŠ€æœ¯æœ‰åŠ©äºæé«˜æ–‡æ¡£çš„ç›¸å…³æ€§ã€‚

Thereâ€™s however an ongoing research in this area and other methods are currently being assessed. For example,

ç„¶è€Œï¼Œè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ä»åœ¨è¿›è¡Œä¸­ï¼Œå…¶ä»–æ–¹æ³•ä¹Ÿåœ¨è¯„ä¼°ä¸­ã€‚ä¾‹å¦‚ï¼š

* Fine-tuning the embedding model å¾®è°ƒåµŒå…¥æ¨¡å‹
* Fine-tuning the LLM directly ç›´æ¥å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹
* Deep embedding adaptors æ·±åº¦åµŒå…¥é€‚é…å™¨
* Deep chunking æ·±åº¦åˆ†å—

***

åœ¨LlamaIndexä¸­ï¼Œæœ‰ä¸€äº›æŠ€æœ¯ä¸æŸ¥è¯¢æ‰©å±•ç´§å¯†ç›¸å…³ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å…¶"Query Pipelines"å’Œ"Query Transformations"åŠŸèƒ½ã€‚

1. **Query Pipelines**ï¼šLlamaIndexå¼•å…¥äº†Query Pipelinesï¼Œè¿™æ˜¯ä¸€ç§å£°æ˜å¼APIï¼Œå…è®¸ç”¨æˆ·ç®€æ´åœ°æ„å»ºä»ç®€å•åˆ°é«˜çº§çš„æŸ¥è¯¢å·¥ä½œæµã€‚è¿™åŒ…æ‹¬äº†å¯¹ç”¨æˆ·æŸ¥è¯¢çš„ç†è§£å’Œè½¬æ¢ï¼ˆå¦‚é‡å†™ã€è·¯ç”±ï¼‰ï¼Œå¯èƒ½è¿˜åŒ…æ‹¬å¤šé˜¶æ®µçš„æ£€ç´¢ç®—æ³•ï¼ˆå¦‚top-kæŸ¥æ‰¾+é‡æ’åºï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…è®¸ä½¿ç”¨æç¤ºå’ŒLLMsä»¥ä¸åŒæ–¹å¼è¿›è¡Œå“åº”åˆæˆã€‚Query Pipelinesæä¾›äº†ä¸€ä¸ªå£°æ˜å¼çš„æŸ¥è¯¢ç¼–æ’æŠ½è±¡ï¼Œå…è®¸ç”¨æˆ·ä»¥æ›´å°‘çš„ä»£ç è¡Œæ•°è¡¨è¾¾å¸¸è§çš„æŸ¥è¯¢å·¥ä½œæµç¨‹ï¼Œå¹¶æé«˜ä»£ç çš„å¯è¯»æ€§ã€‚[LlamaIndex](https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537)
2. **Query Transformations**ï¼šLlamaIndexå…è®¸åœ¨ç´¢å¼•ç»“æ„ä¸Šæ‰§è¡ŒæŸ¥è¯¢è½¬æ¢ï¼Œè¿™äº›è½¬æ¢æ¨¡å—ä¼šå°†ä¸€ä¸ªæŸ¥è¯¢è½¬æ¢ä¸ºå¦ä¸€ä¸ªæŸ¥è¯¢ã€‚è¿™å¯ä»¥æ˜¯å•æ­¥çš„ï¼Œå³åœ¨æ‰§è¡Œç´¢å¼•æŸ¥è¯¢ä¹‹å‰è¿è¡Œä¸€æ¬¡è½¬æ¢ï¼›ä¹Ÿå¯ä»¥æ˜¯å¤šæ­¥çš„ï¼Œå³è½¬æ¢æŸ¥è¯¢ã€å¯¹ç´¢å¼•æ‰§è¡ŒæŸ¥è¯¢ï¼Œç„¶åå¯èƒ½å†æ¬¡è½¬æ¢ç»“æœã€‚[LlamaIndex](https://docs.llamaindex.ai/en/stable/optimizing/advanced\_retrieval/query\_transformations/)

ä¾‹å¦‚ï¼Œåœ¨Query Pipelinesä¸­ï¼Œå¯ä»¥é€šè¿‡ä¸¤ä¸ªæç¤ºï¼ˆpromptsï¼‰ä¹‹åè¿›è¡Œæ£€ç´¢ï¼Œå…¶ä¸­ä¸€ä¸ªæç¤ºç”¨äºåº”ç”¨æŸ¥è¯¢é‡å†™ï¼Œå¦ä¸€ä¸ªç”Ÿæˆå‡è®¾ç­”æ¡ˆä»¥ä¸°å¯Œæ£€ç´¢ï¼ˆç§°ä¸ºHyDEï¼‰ã€‚è¿™å¯ä»¥å»ºç«‹ä¸ºé¡ºåºé“¾ï¼Œå› ä¸ºæ¯ä¸ªæç¤ºåªæ¥å—ä¸€ä¸ªè¾“å…¥ï¼ŒQueryPipelineå°†è‡ªåŠ¨å°†LLMè¾“å‡ºé“¾å…¥æç¤ºï¼Œç„¶åé“¾å…¥LLMï¼ˆ[Towards AI](https://towardsai.net/p/machine-learning/llamaindex-query-pipelines-quickstart-guide-to-the-declarative-query-api)ï¼‰ã€‚

è¿™äº›ç‰¹æ€§å’ŒæŠ€æœ¯åœ¨LlamaIndexä¸­ä¸ºå¼€å‘è€…æä¾›äº†çµæ´»è€Œå¼ºå¤§çš„å·¥å…·ï¼Œä»¥ä¼˜åŒ–å’Œå¢å¼ºæŸ¥è¯¢å¤„ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠæŸ¥è¯¢æ‰©å±•ä»¥æé«˜æ£€ç´¢æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢ã€‚



<figure><img src="../.gitbook/assets/WechatIMG255.jpg" alt=""><figcaption></figcaption></figure>
